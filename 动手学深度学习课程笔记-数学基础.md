

## 动手学深度学习 -1 

![image1](img\image-20230321181239508.png)

计算机视觉：概率模型 机器学习

机器学习->深度学习

- 图片分类

- 物体检测：知道图片中有什么内容

- 物体分割：每个像素描述的是什么

- 样式迁移：样式图片和内容图片的合成

- 人脸合成

- 文字生成图片
- 文字生成 GPT-3
- 无人驾驶
- 案例研究：广告点击-推荐系统

![image-20230318155209008](img\image-20230318155209008.png)

broadcast广播机制：

两个维度相同的张量相加会调成一样的长度

#### 线代简单操作

![image-20230320205228729](img\image-20230320205228729.png)

点乘

![image-20230320205430420](img\image-20230320205430420.png)

点乘的空间意义：一个向量在另一个向量方向上的投影

叉乘：右手定则，垂直于v和w两个向量的新向量，长度是v和w围成的平行四边形面积（行列式）

[参考视频：线性代数的本质](https://www.bilibili.com/video/BV1ys411472E/?p=12)

#### 范数

![image-20230320210216970](img\image-20230320210216970.png)

[参考视频：范数计算速成](https://www.bilibili.com/video/BV1WZ4y1V7Vo/)

#### 指定求和汇总张量的轴

按维度求和：sum(axis=1,keepdims=True) 维持维度、

![![img](img\image-20230320220746972.png)](img\image-20230320220746972.png)

![img](img\v2-e359e37a0411b04598a5f3fbdc4f1960_r.jpg)

#### 矩阵计算

梯度的几何意义：垂直于等高线方向，即梯度下降最快的方向

分子式：分母转置 行向量偏导形式

分母式：分子转置 

![image-20230321000107054](img\image-20230321000107054.png)

#### 矩阵计算练习

1. 证明一个矩阵A的转置的转置是A，即(A⊤)⊤=A。

   ```
   A==A.T.T
   ```

   ```
   tensor([[True, True, True, True],
           [True, True, True, True],
           [True, True, True, True],
           [True, True, True, True],
           [True, True, True, True]])
   ```

2. 给出两个矩阵A和B，证明“它们转置的和”等于“它们和的转置”，即A⊤+B⊤=(A+B)⊤。

   

3. 给定任意方阵A，A+A⊤总是对称的吗?为什么?

   是的，(A+A.T).T=A.T+A.T.T=A+A.T

4. 本节中定义了形状(2,3,4)的张量`X`。`len(X)`的输出结果是什么？

   2

5. 对于任意形状的张量`X`,`len(X)`是否总是对应于`X`特定轴的长度?这个轴是什么?

   axis=0的长度

6. 运行`A/A.sum(axis=1)`，看看会发生什么。请分析一下原因？

   ```
   RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1
   ```

   维度不同，A是二维，A.sum(axis=1)是一维，可以使用keepdims=True

7. 考虑一个具有形状(2,3,4)的张量，在轴0、1、2上的求和输出是什么形状?

   ```
   D=torch.arange(24,dtype=torch.float32).reshape(2,3,4)
   D.sum(axis=0),D.sum(axis=1),tousum(axis=2)
   ```

   ```
   tensor([[[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.]],
   
           [[12., 13., 14., 15.],
            [16., 17., 18., 19.],
            [20., 21., 22., 23.]]])
   ```

   （3,4），（2,4），（2,3）

8. 为`linalg.norm`函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?

标量

### 2.4 微积分

自动求导的原理：计算图

![image-20230321144335060](img\image-20230321144335060.png)

每个圈表示一个输入/操作

求导：

- 公式求导
- 数值求导（逼近）
- 自动求导

![image-20230321144551826](img\image-20230321144551826.png)

正向累积：从x出发，计算复杂度O(n)，内存复杂度O(n)，需要存所有向量的中间结果

反向累积-反向传递：从y出发，计算复杂度O(n），内存复杂度O(1)，每计算一个变量梯度要重新扫描整棵树

```
#require grad 提前
m=torch.arange(4,dtype=torch.float32)
m.requires_grad_(True)
m
```

Out:

```
tensor([0., 1., 2., 3.], requires_grad=True)
```

```
y=2*torch.dot(x,x)
y
```

Out[72]:

```
tensor(28., grad_fn=<MulBackward0>)
```

说明y是x累积得来

通过调用反向传播自动计算y关于x的梯度：

```
y.backward()
x.grad
```

```
tensor([ 0.,  4.,  8., 12.])
```

梯度会积累，使用x.grad.zero_()清空

QA:隐式构造和显式构造：

显式计算先给公式再给值 

正向算y值，反向求梯度
